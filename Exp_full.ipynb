{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6468e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as alg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e078105",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self, configs):\n",
    "        self.typ = configs['typ']\n",
    "        self.state = configs['init_state']\n",
    "        self.H = configs['H']\n",
    "        self.init_state = configs['init_state']\n",
    "        self.state_space = configs['state_space']\n",
    "        self.action_space = configs['action_space']     \n",
    "            \n",
    "        if self.typ == 'river':\n",
    "            self.S = len(self.state_space)\n",
    "        \n",
    "    def next_state(self, action):\n",
    "        typ = self.typ\n",
    "        ###here to specify how to transfer\n",
    "        if typ == 'river':\n",
    "            if self.state == 0:\n",
    "                if action == 0:\n",
    "                    self.state = 1\n",
    "                else:\n",
    "                    p = [0.1, 0.9]\n",
    "                    self.state = np.random.choice([0,1], p = p)\n",
    "                return\n",
    "            if self.state == self.S -1:\n",
    "                if action == 0:\n",
    "                    self.state = self.S-2\n",
    "                else:\n",
    "                    p = [0.05, 0.95]\n",
    "                    self.state = np.random.choice([self.S-2, self.S-1], p = p)\n",
    "                return\n",
    "            else:\n",
    "                if action == 0:\n",
    "                    self.state -=1\n",
    "                else:\n",
    "                    p = [0.05, 0.05, 0.9]\n",
    "                    self.state = np.random.choice([self.state-1, self.state, self.state+1], p = p)\n",
    "                return\n",
    "                \n",
    "                    \n",
    "            \n",
    "    def reset(self):\n",
    "        self.state = self.init_state\n",
    "        print('reset')\n",
    "        return \n",
    "        \n",
    "    \n",
    "    def reward(self, state, action):\n",
    "        typ = self.typ\n",
    "        if typ == 'river':\n",
    "            if state == 0:\n",
    "                if action == 0:\n",
    "                    return 0.005\n",
    "            if state == self.S - 1:\n",
    "                if action == 1:\n",
    "                    return 1\n",
    "            return 0\n",
    "\n",
    "    def phi(self, state, action):\n",
    "        ####given state and action, return a matrix in shape d times S\n",
    "        ###state is 0,1,2,..., action is arbitrary \n",
    "        \n",
    "        if self.typ == 'river':\n",
    "            ##here we treat river as a 3-dimensional linear mixture MDP\n",
    "            ###should return 3 times S\n",
    "            rr = np.zeros((3, self.S))\n",
    "            if action == 1:               \n",
    "                if state == 0:\n",
    "                    rr[1,0] = 1\n",
    "                    rr[2,0] = 1\n",
    "                    rr[0,1] = 1\n",
    "                if state == self.S-1:\n",
    "                    rr[0,-1] = 1\n",
    "                    rr[1,-2] = 1\n",
    "                    rr[2,-2] = 1\n",
    "                if 0<state<self.S - 1:\n",
    "                    rr[0,state+1] = 1\n",
    "                    rr[1, state] = 1\n",
    "                    rr[2, state-1] = 1\n",
    "            else:\n",
    "                ss = max(0,state-1)\n",
    "                rr[0,ss] = 1\n",
    "                rr[1,ss] = 1\n",
    "                rr[2,ss] = 1\n",
    "                \n",
    "            return rr\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65fd33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VTR_HF:\n",
    "    def __init__(self, configs):\n",
    "        self.i = 0\n",
    "        \n",
    "        \n",
    "        self.dim = configs['dim']\n",
    "        self.M = configs['M']\n",
    "        self.lam = configs['lam']\n",
    "        self.state_space = configs['state_space']\n",
    "        self.action_space = configs['action_space']\n",
    "        self.H = configs['H']\n",
    "        self.beta = configs['beta']   \n",
    "        self.alpha = configs['alpha']\n",
    "        self.gamma = configs['gamma']\n",
    "\n",
    "        \n",
    "        ###the length of the following list is of M\n",
    "        self.phi_list = []\n",
    "        self.theta_list = [np.zeros((self.dim, 1))]*self.M        \n",
    "        self.tilde_Sigma_list = [np.identity(self.dim)]*self.M\n",
    "        self.b_list = [np.zeros((self.dim, 1))]*self.M\n",
    "        self.hat_Sigma_list = [np.identity(self.dim)]*self.M\n",
    "        self.v_func = 0\n",
    "        ###self.v_func is the next_value function in shape S times H (v_2, _3, ..., v_H+1)\n",
    "        \n",
    "    def Q(self, v_next, reward, phi, state, action):\n",
    "        ###here v_next is a S times 1 shape vector\n",
    "        phiv = phi(state, action)@v_next        \n",
    "        theta = self.theta_list[0]\n",
    "        ####phiv in shape d \\times 1\n",
    "        v_action = reward(state, action) + theta.transpose()@phiv + \\\n",
    "        self.beta*np.sqrt(phiv.transpose()@alg.solve(self.hat_Sigma_list[0], phiv))\n",
    "        v_action = np.clip(v_action, 0,1)[0,0]       \n",
    "        return v_action\n",
    "        \n",
    "    def update_v(self, reward, phi):\n",
    "        ##get current v function (H number) from line 4 to 6 in Algorithm 1\n",
    "        ###v_func is in shape S times H\n",
    "        v_func = np.zeros((len(self.state_space), 1))\n",
    "        for h in range(self.H, 1, -1):\n",
    "            v= np.zeros((len(self.state_space),1))\n",
    "            v_next = v_func[:,0:1]\n",
    "            \n",
    "            for state in self.state_space:\n",
    "                all_action = []\n",
    "                for action in self.action_space:\n",
    "                    v_action = self.Q(v_next, reward, phi, state, action)\n",
    "                    all_action.append(v_action)\n",
    "                v[state,0] = max(all_action)\n",
    "            v_func = np.concatenate((v, v_func), axis = 1)\n",
    "        \n",
    "        ###v_func is in shape S times H\n",
    "        self.v_func = v_func\n",
    "        return \n",
    "    \n",
    "    def update_hat(self):\n",
    "           \n",
    "        self.hat_Sigma_list = self.tilde_Sigma_list.copy()     \n",
    "        self.theta_list = []            \n",
    "        for m in range(self.M):\n",
    "            self.theta_list.append(alg.solve(self.hat_Sigma_list[m], self.b_list[m]))\n",
    "        return\n",
    "    \n",
    "    def select_action(self, state, h, reward, phi):\n",
    "        v = self.v_func[:,h:(h+1)]\n",
    "        all_action = {}\n",
    "        for i in range(len(self.action_space)):\n",
    "            action = self.action_space[i]\n",
    "            all_action[i] = self.Q(v, reward, phi, state, action)\n",
    "        \n",
    "        actions = []\n",
    "\n",
    "        for i in range(len(self.action_space)):\n",
    "            if all_action[i] == all_action[max(all_action)]:\n",
    "                actions.append(i)\n",
    "        \n",
    "        action = np.random.choice(actions)\n",
    "        \n",
    "        return self.action_space[action]\n",
    "    \n",
    "    def update(self,state, action, next_state, h, phi):\n",
    "        ###from line 11 to line 14\n",
    "        ####line 11\n",
    "        \n",
    "        self.phi_list = []\n",
    "        for m in range(self.M):\n",
    "            vm = self.v_func[:,h:(h+1)]**(2**m)\n",
    "            phiv = phi(state, action)@vm \n",
    "            self.phi_list.append(phiv)\n",
    "        ####line 12\n",
    "        bar_sigma = []\n",
    "        for m in range(self.M-1):\n",
    "            var_1 = np.clip(self.phi_list[m+1].transpose()@self.theta_list[m+1],0,1)\n",
    "            var_2 =np.clip(self.phi_list[m].transpose()@self.theta_list[m],0,1)\n",
    "            variance = var_1 - var_2\n",
    "            \n",
    "            E_1 = np.clip(2*self.beta*np.sqrt(self.phi_list[m].transpose()@alg.solve(self.hat_Sigma_list[m], self.phi_list[m])),a_min = None, a_max= 1)\n",
    "            E_2 = np.clip(self.beta*np.sqrt(self.phi_list[m+1].transpose()@alg.solve(self.hat_Sigma_list[m+1], self.phi_list[m+1])),a_min = None, a_max = 1)\n",
    "            E = E_1 + E_2\n",
    "\n",
    "            var_1 = (variance + E)[0,0]\n",
    "            var_2 = self.alpha**2\n",
    "            var_3 = self.gamma**2*np.sqrt(self.phi_list[m].transpose()@alg.solve(self.tilde_Sigma_list[m], self.phi_list[m]))\n",
    "\n",
    "            variance = max(var_1, var_2, var_3)\n",
    "            bar_sigma.append(variance)\n",
    "        \n",
    "        variance = self.gamma**2*np.sqrt(self.phi_list[-1].transpose()@alg.solve(self.tilde_Sigma_list[-1], self.phi_list[-1]))\n",
    "        bar_sigma.append(max(1, self.alpha**2, variance))\n",
    "        ###bar_sigma is a list of length M\n",
    "        \n",
    "        ###line 13 and 14\n",
    "        for m in range(self.M):\n",
    "            self.tilde_Sigma_list[m] = self.tilde_Sigma_list[m] + self.phi_list[m]@self.phi_list[m].transpose()/bar_sigma[m]\n",
    "            self.b_list[m] = self.b_list[m] + self.v_func[next_state,h]**(2**m)*self.phi_list[m]/bar_sigma[m]\n",
    "       \n",
    "        return\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f1fd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearning:\n",
    "    \n",
    "####Q learning enjoys the state space [0,1,2,...] and action space [0,1,2,...]\n",
    "    def __init__(self, configs):\n",
    "        self.i = 0\n",
    "        \n",
    "        \n",
    "\n",
    "        self.state_space = configs['state_space']\n",
    "        self.action_space = configs['action_space']\n",
    "        self.H = configs['H']\n",
    "        self.beta = configs['Q_beta']   \n",
    "        \n",
    "        self.Q_list = self.H*np.ones((self.H, len(self.state_space), len(self.action_space)))\n",
    "        self.V_list = np.zeros((self.H+1, len(self.state_space)))\n",
    "        ####self.num is the count in shape H times S times A\n",
    "        self.num = np.zeros((self.H, len(self.state_space), len(self.action_space)))\n",
    "        ###self.v_func is the next_value function in shape S times H (v_2, _3, ..., v_H+1)\n",
    "\n",
    "    \n",
    "    def select_action(self, state, h):\n",
    "        q = self.Q_list[h, state, :]\n",
    "        \n",
    "        actions = []\n",
    "\n",
    "        for i in range(len(q)):\n",
    "            if q[i] == max(q):\n",
    "                actions.append(i)\n",
    "        \n",
    "        action = np.random.choice(actions)        \n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def update(self,state, action, next_state, h, reward):\n",
    "        self.num[h, state, action] += 1\n",
    "        beta = self.beta/np.sqrt(self.num[h, state, action])\n",
    "        alpha = (self.H+1)/(self.H + self.num[h, state, action])\n",
    "        ####update Q\n",
    "        self.Q_list[h, state, action] = (1-alpha)*self.Q_list[h, state, action] + alpha*(reward(state,action) + \\\n",
    "                                        self.V_list[h+1, next_state] + beta)\n",
    "        self.V_list[h, state] = min(self.H, max(self.Q_list[h, state, :]))\n",
    "        \n",
    "\n",
    "        return\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a475f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGS = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c965e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "\n",
    "configs = {}\n",
    "configs['typ'] = 'river'\n",
    "\n",
    "#########this is the mdp part\n",
    "configs['state_space'] = [0, 1,2,3,4,5]\n",
    "configs['dim'] = 3\n",
    "configs['action_space'] = [0,1]\n",
    "\n",
    "configs['init_state'] = 0\n",
    "configs['H'] = 500\n",
    "\n",
    "#####this is the vtrhf part####\n",
    "configs['lam'] = 0.001\n",
    "configs['beta'] = 1\n",
    "configs['alpha'] = 0.01\n",
    "configs['gamma'] = 0.5\n",
    "configs['M'] = 4\n",
    "\n",
    "######################################\n",
    "#####this is the Q learning part#############\n",
    "configs['Q_beta'] = 0.05\n",
    "\n",
    "CONFIGS['river'] = configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49548512-a97b-42a4-afc2-8e5e31bf7dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### HF-UCRL-VTR+ with M = 4 ##############\n",
    "para_set = [(0.001, 1, 0.01, 0.5, 4)]\n",
    "\n",
    "K = 500\n",
    "typ = 'river'\n",
    "for para in para_set:\n",
    "    (CONFIGS['river']['lam'], CONFIGS['river']['beta'], CONFIGS['river']['alpha'], CONFIGS['river']['gamma'], CONFIGS['river']['M']) = para\n",
    "    for ii in range(10):  \n",
    "        mdp = MDP(CONFIGS[typ])\n",
    "        vtr_hf = VTR_HF(CONFIGS[typ])\n",
    "\n",
    "        result = [0]\n",
    "        for k in range(K):\n",
    "            mdp.reset()\n",
    "\n",
    "            ####first get value function estimate\n",
    "            vtr_hf.update_v(mdp.reward, mdp.phi)\n",
    "            print('episode:', k)\n",
    "            ###v is of S times H shape\n",
    "            rr = 0\n",
    "            visited = 0\n",
    "            for h in range(mdp.H):        \n",
    "                if mdp.state == 5:\n",
    "                    visited += 1\n",
    "                state = mdp.state\n",
    "                action = vtr_hf.select_action(mdp.state, h, mdp.reward, mdp.phi)\n",
    "                rr += mdp.reward(state, action)\n",
    "                mdp.next_state(action)\n",
    "                next_state = mdp.state\n",
    "                vtr_hf.update(state, action, next_state, h, mdp.phi)\n",
    "            vtr_hf.update_hat()\n",
    "            result.append((rr + result[-1]*(len(result)-1))/len(result))\n",
    "            print(result[-1])\n",
    "\n",
    "        file = './'+ typ+'/full_'+str(CONFIGS[typ]['lam']) + '_'+str(CONFIGS[typ]['beta'])\\\n",
    "        + '_' + str(CONFIGS[typ]['alpha']) +'_'+ str(CONFIGS[typ]['gamma']) + '_' + str(ii)\n",
    "        np.savetxt(file, np.array(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81985260-51d8-44c0-bd00-8f7c26dce870",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########'HF-UCRL-VTR+ with M = 2'#############\n",
    "para_set = [(0.001, 1, 0.01, 0.5, 2)]\n",
    "\n",
    "K = 500\n",
    "typ = 'river'\n",
    "for para in para_set:\n",
    "    (CONFIGS['river']['lam'], CONFIGS['river']['beta'], CONFIGS['river']['alpha'], CONFIGS['river']['gamma'], CONFIGS['river']['M']) = para\n",
    "    for ii in range(10):  \n",
    "        mdp = MDP(CONFIGS[typ])\n",
    "        vtr_hf = VTR_HF(CONFIGS[typ])\n",
    "\n",
    "        result = [0]\n",
    "        for k in range(K):\n",
    "            mdp.reset()\n",
    "\n",
    "            ####first get value function estimate\n",
    "            vtr_hf.update_v(mdp.reward, mdp.phi)\n",
    "            print('episode:', k)\n",
    "            ###v is of S times H shape\n",
    "            rr = 0\n",
    "            visited = 0\n",
    "            for h in range(mdp.H):        \n",
    "                if mdp.state == 5:\n",
    "                    visited += 1\n",
    "                state = mdp.state\n",
    "                action = vtr_hf.select_action(mdp.state, h, mdp.reward, mdp.phi)\n",
    "                rr += mdp.reward(state, action)\n",
    "                mdp.next_state(action)\n",
    "                next_state = mdp.state\n",
    "                vtr_hf.update(state, action, next_state, h, mdp.phi)\n",
    "            vtr_hf.update_hat()\n",
    "            result.append((rr + result[-1]*(len(result)-1))/len(result))\n",
    "            print(result[-1])\n",
    "\n",
    "        file = './'+ typ+'/singlem_'+str(CONFIGS[typ]['lam']) + '_'+str(CONFIGS[typ]['beta'])\\\n",
    "        + '_' + str(CONFIGS[typ]['alpha']) +'_'+ str(CONFIGS[typ]['gamma']) + '_' + str(ii)\n",
    "        np.savetxt(file, np.array(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c39474d-0a34-462c-a2d4-bff166077514",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########'UCRL-VTR+'############\n",
    "para_set = [(0.001, 1, 0.01, 0, 2)]\n",
    "\n",
    "K = 500\n",
    "typ = 'river'\n",
    "for para in para_set:\n",
    "    (CONFIGS['river']['lam'], CONFIGS['river']['beta'], CONFIGS['river']['alpha'], CONFIGS['river']['gamma'], CONFIGS['river']['M']) = para\n",
    "    for ii in range(10):  \n",
    "        mdp = MDP(CONFIGS[typ])\n",
    "        vtr_hf = VTR_HF(CONFIGS[typ])\n",
    "\n",
    "        result = [0]\n",
    "        for k in range(K):\n",
    "            mdp.reset()\n",
    "\n",
    "            ####first get value function estimate\n",
    "            vtr_hf.update_v(mdp.reward, mdp.phi)\n",
    "            print('episode:', k)\n",
    "            ###v is of S times H shape\n",
    "            rr = 0\n",
    "            visited = 0\n",
    "            for h in range(mdp.H):        \n",
    "                if mdp.state == 5:\n",
    "                    visited += 1\n",
    "                state = mdp.state\n",
    "                action = vtr_hf.select_action(mdp.state, h, mdp.reward, mdp.phi)\n",
    "                rr += mdp.reward(state, action)\n",
    "                mdp.next_state(action)\n",
    "                next_state = mdp.state\n",
    "                vtr_hf.update(state, action, next_state, h, mdp.phi)\n",
    "            vtr_hf.update_hat()\n",
    "            result.append((rr + result[-1]*(len(result)-1))/len(result))\n",
    "            print(result[-1])\n",
    "\n",
    "        file = './'+ typ+'/nouncern_'+str(CONFIGS[typ]['lam']) + '_'+str(CONFIGS[typ]['beta'])\\\n",
    "        + '_' + str(CONFIGS[typ]['alpha']) +'_'+ str(CONFIGS[typ]['gamma']) + '_' + str(ii)\n",
    "        np.savetxt(file, np.array(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f3540f-bda1-4a9c-971b-63923939eea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########'UCRL-VTR'############\n",
    "para_set = [(0.001, 1, 0.01, 0, 1)]\n",
    "\n",
    "K = 500\n",
    "typ = 'river'\n",
    "for para in para_set:\n",
    "    (CONFIGS['river']['lam'], CONFIGS['river']['beta'], CONFIGS['river']['alpha'], CONFIGS['river']['gamma'], CONFIGS['river']['M']) = para\n",
    "    for ii in range(10):  \n",
    "        mdp = MDP(CONFIGS[typ])\n",
    "        vtr_hf = VTR_HF(CONFIGS[typ])\n",
    "\n",
    "        result = [0]\n",
    "        for k in range(K):\n",
    "            mdp.reset()\n",
    "\n",
    "            ####first get value function estimate\n",
    "            vtr_hf.update_v(mdp.reward, mdp.phi)\n",
    "            print('episode:', k)\n",
    "            ###v is of S times H shape\n",
    "            rr = 0\n",
    "            visited = 0\n",
    "            for h in range(mdp.H):        \n",
    "                if mdp.state == 5:\n",
    "                    visited += 1\n",
    "                state = mdp.state\n",
    "                action = vtr_hf.select_action(mdp.state, h, mdp.reward, mdp.phi)\n",
    "                rr += mdp.reward(state, action)\n",
    "                mdp.next_state(action)\n",
    "                next_state = mdp.state\n",
    "                vtr_hf.update(state, action, next_state, h, mdp.phi)\n",
    "            vtr_hf.update_hat()\n",
    "            result.append((rr + result[-1]*(len(result)-1))/len(result))\n",
    "            print(result[-1])\n",
    "\n",
    "        file = './'+ typ+'/hoeffding_'+str(CONFIGS[typ]['lam']) + '_'+str(CONFIGS[typ]['beta'])\\\n",
    "        + '_' + str(CONFIGS[typ]['alpha']) +'_'+ str(CONFIGS[typ]['gamma']) + '_' + str(ii)\n",
    "        np.savetxt(file, np.array(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb71e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########run Q learning      \n",
    "K = 500\n",
    "para_set = [0.005]\n",
    "for para in para_set:\n",
    "    typ = 'river'\n",
    "    CONFIGS['river']['Q_beta'] = para\n",
    "    for ii in range(10):  \n",
    "        mdp = MDP(CONFIGS[typ])\n",
    "        qlearning = Qlearning(CONFIGS[typ])\n",
    "\n",
    "        result = [0]\n",
    "        for k in range(K):\n",
    "            mdp.reset()\n",
    "\n",
    "            ####first get value function estimate\n",
    "            print('episode:', k)\n",
    "            ###v is of S times H shape\n",
    "            rr = 0\n",
    "            for h in range(mdp.H):        \n",
    "                state = mdp.state\n",
    "                action = qlearning.select_action(mdp.state, h)\n",
    "                rr += mdp.reward(state, action)\n",
    "                mdp.next_state(action)\n",
    "                next_state = mdp.state\n",
    "                qlearning.update(state, action, next_state, h, mdp.reward)\n",
    "\n",
    "\n",
    "            result.append((rr + result[-1]*(len(result)-1))/len(result))\n",
    "            print(result[-1])\n",
    "\n",
    "        file = './' + typ +'/qlearning_' + str(para) + '_'+str(ii)\n",
    "        np.savetxt(file, np.array(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f2830f-0e42-42fc-8910-1186ac676793",
   "metadata": {},
   "outputs": [],
   "source": [
    "####run random\n",
    "\n",
    "K = 500\n",
    "typ = 'river'\n",
    "for ii in range(100):  \n",
    "    mdp = MDP(CONFIGS[typ])\n",
    "    result = [0]\n",
    "    for k in range(K):\n",
    "        mdp.reset()\n",
    "\n",
    "        ####first get value function estimate\n",
    "        print('episode:', k)\n",
    "        ###v is of S times H shape\n",
    "        rr = 0\n",
    "        visited = 0\n",
    "        for h in range(mdp.H):        \n",
    "            if mdp.state == 5:\n",
    "                visited += 1\n",
    "            state = mdp.state\n",
    "            action = np.random.choice(mdp.action_space)\n",
    "            rr += mdp.reward(state, action)\n",
    "            mdp.next_state(action)\n",
    "            next_state = mdp.state\n",
    "\n",
    "        result.append((rr + result[-1]*(len(result)-1))/len(result))\n",
    "        print(result[-1])\n",
    "\n",
    "    file = './'+ typ+'/random_'+ str(ii)\n",
    "    np.savetxt(file, np.array(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e3ed4a-777b-49d5-8a34-9c6cd082afb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
